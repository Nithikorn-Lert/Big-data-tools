{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark - SparkSQL CookNotebook part 1 (Python, Scala, Java).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MyXFGMYhdR63",
        "MDVi2pWKVcjZ",
        "_EoPKdPLG-qZ",
        "9x_eK_0LVo8_",
        "57prvM1tLZFb",
        "h5tcT1MZpgoC",
        "_aSV0XAphPwY",
        "SHPv2Jb9dHyO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7yB5QEwFeOR",
        "colab_type": "text"
      },
      "source": [
        "Data Wrangling part 1\n",
        "\n",
        "Since there are several ways to access the data in Spark dataframe. In this note book we will show how to access the data in several method in three programming languages which are Python, Scala and Java.　Because Scala and Java are not compatible with Jupyter notebook. So, we will use the result from IntelliJ instead.\n",
        "\n",
        "このノートブックの目的はSparkの色んな方法でデータフレームをアクセスします。\n",
        "Sparkデータフレームをラングリングするのは様々な方法で初心者として混乱しやすいです。\n",
        "そのため、このノートブックはアクセス方法をできるだけまとめます。それに、このノートブックはPython(PySpark)だけではなく、JavaとScalaにも対応します。\n",
        "\n",
        "\n",
        "\n",
        "Ref:\n",
        "- [1] https://x1.inkenkun.com/archives/1114#SELECT\n",
        "- [2] http://mogile.web.fc2.com/spark/spark210/sql-programming-guide.html\n",
        "- [3] superstore dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyXFGMYhdR63",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOSc-mXClb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71yx1CQm05wi",
        "colab_type": "code",
        "outputId": "90ef0d22-12c8-45ac-aff2-98cf94586322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!git clone https://github.com/mikemooreviz/superstore.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'superstore'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 9\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOy2xT0dCnX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession\\\n",
        "         .builder\\\n",
        "         .master(\"local[*]\")\\\n",
        "         .appName(\"sample_app\")\\\n",
        "         .enableHiveSupport()\\\n",
        "         .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-M7nuhd2TpQ",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        import org.apache.spark.sql.SparkSession\n",
        "        import org.apache.spark.SparkContext\n",
        "        import org.apache.spark.SparkConf\n",
        "        import org.apache.spark.sql.functions.column\n",
        "        import org.apache.spark.sql.functions.col\n",
        "        import org.apache.spark.sql.functions.when\n",
        "        import org.apache.spark.sql.functions.map\n",
        "        import org.apache.spark.sql.functions.round\n",
        "\n",
        "        val spark = SparkSession\n",
        "                    .builder()\n",
        "                    .master(\"local[*]\")\n",
        "                    .appName(\"sample_app\")\n",
        "                    .getOrCreate()\n",
        "                 import spark.implicits._\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cSKkv83Cqzw",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        import org.apache.spark.sql.SparkSession;\n",
        "        import org.apache.spark.api.java.JavaRDD;\n",
        "        import org.apache.spark.api.java.JavaSparkContext;\n",
        "        import org.apache.spark.SparkConf;\n",
        "        import org.apache.spark.sql.Dataset;\n",
        "        import org.apache.spark.sql.Row;\n",
        "        import scala.collection.Map;\n",
        "        import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "        SparkSession spark = SparkSession\n",
        "                            .builder()\n",
        "                            .master(\"local[*]\")\n",
        "                            .appName(\"sample_app\")\n",
        "                            .getOrCreate();"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r012leY9xiIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "def shape(self):\n",
        "    return (self.count(), len(self.columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdy0CcqrOxfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_py = spark.read.csv(\"superstore/superstore.csv\", header=True)\n",
        "\n",
        "# register SQL-command df like table named df_SQL (single sessionj)\n",
        "# SQL言語としてテーブルで扱うことになる (シングルセッション)\n",
        "df_py.createOrReplaceTempView(\"df_py_SQL\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUhfG2Au5Ajk",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        val df_scala = spark\n",
        "                .read.format(\"csv\")\n",
        "                .option(\"header\", \"true\")\n",
        "                .load(\"C:/Users/CU - teminal/Desktop/Spark/Superstore.csv\")\n",
        "            \n",
        "            df_scala.createOrReplaceTempView(\"df_scala_SQL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnCO9X3zCwPo",
        "colab_type": "text"
      },
      "source": [
        "    // java         \n",
        "        // review: List<String> my_list = new ArrayList<String>();\n",
        "        // function List type (generally you will see <T> which is abbreviated form of Type) String\n",
        "\n",
        "        Dataset<Row> df_java = spark.read()\n",
        "                .format(\"csv\")\n",
        "                .option(\"header\", \"true\")\n",
        "                .load(\"C:/Users/CU - teminal/Desktop/Spark/Superstore.csv\");\n",
        "\n",
        "        df_java.createOrReplaceTempView(\"df_java_SQL\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwMOgwEBRNmE",
        "colab_type": "code",
        "outputId": "e6804f2d-0426-4d91-d4cb-39b4dfd99748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "df_py.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Row ID: string (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Order Date: string (nullable = true)\n",
            " |-- Ship Date: string (nullable = true)\n",
            " |-- Ship Mode: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Segment: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Postal Code: string (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Product ID: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Sub-Category: string (nullable = true)\n",
            " |-- Product Name: string (nullable = true)\n",
            " |-- Sales: string (nullable = true)\n",
            " |-- Quantity: string (nullable = true)\n",
            " |-- Discount: string (nullable = true)\n",
            " |-- Profit: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df67XKJg6Egz",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        df_scala.printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ROBSQCCyeG",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        df_java.printSchema();"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdHSopKfj6OK",
        "colab_type": "code",
        "outputId": "a7f5f3bd-a4a4-47af-f90a-53788d8c839f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(shape(df_py))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9994, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmipOujA62r5",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        val r = df_scala.count\n",
        "        val c = df_scala.columns.length\n",
        "        print( s\"$r, $c\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EREPQ9f4C0nY",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        System.out.println(df_java.columns().length + \",\" + df_java.count());"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDVi2pWKVcjZ",
        "colab_type": "text"
      },
      "source": [
        "### ```.select```......```.where```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wNIQBtterhf",
        "colab_type": "code",
        "outputId": "817c9c20-b94d-4112-d85b-f6f29d3d49fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "# Pyspark approach\n",
        "from pyspark.sql import functions as F\n",
        "df_py.select(F.col(\"Customer Name\")).show(5)\n",
        "\n",
        "# Pyspark-SQL style approach\n",
        "query = \"\"\" SELECT `Customer Name`\n",
        "            FROM   df_py_SQL\n",
        "        \"\"\"\n",
        "spark.sql(query).show(5) \n",
        "\n",
        "# Pythonic approach\n",
        "df_py.select(df_py[\"Customer Name\"], \n",
        "             df_py[6]).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|  Customer Name|\n",
            "+---------------+\n",
            "|    Claire Gute|\n",
            "|    Claire Gute|\n",
            "|Darrin Van Huff|\n",
            "| Sean O'Donnell|\n",
            "| Sean O'Donnell|\n",
            "+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+\n",
            "|  Customer Name|\n",
            "+---------------+\n",
            "|    Claire Gute|\n",
            "|    Claire Gute|\n",
            "|Darrin Van Huff|\n",
            "| Sean O'Donnell|\n",
            "| Sean O'Donnell|\n",
            "+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+---------------+\n",
            "|  Customer Name|  Customer Name|\n",
            "+---------------+---------------+\n",
            "|    Claire Gute|    Claire Gute|\n",
            "|    Claire Gute|    Claire Gute|\n",
            "|Darrin Van Huff|Darrin Van Huff|\n",
            "| Sean O'Donnell| Sean O'Donnell|\n",
            "| Sean O'Donnell| Sean O'Donnell|\n",
            "+---------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzywHZ8A_N27",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        df_scala.select($\"Customer name\", \n",
        "                col(\"Customer name\"),\n",
        "                column(\"Customer name\") as 'CUSTOMER,\n",
        "                'Profit // allow for non-space column\n",
        "                ).show(3)\n",
        "\n",
        "        df_scala.createOrReplaceTempView(\"df_scala_SQL\")\n",
        "        val query = \"\"\"\n",
        "                    SELECT `Customer name`\n",
        "                    FROM   df_scala_SQL\n",
        "                    \"\"\"\n",
        "        spark.sql(query).show(3)\n",
        "        ///////////////////////////////////////\n",
        "        +---------------+---------------+---------------+-------+\n",
        "        |  Customer name|  Customer name|       CUSTOMER| Profit|\n",
        "        +---------------+---------------+---------------+-------+\n",
        "        |    Claire Gute|    Claire Gute|    Claire Gute|41.9136|\n",
        "        |    Claire Gute|    Claire Gute|    Claire Gute|219.582|\n",
        "        |Darrin Van Huff|Darrin Van Huff|Darrin Van Huff| 6.8714|\n",
        "        +---------------+---------------+---------------+-------+\n",
        "\n",
        "        +---------------+\n",
        "        |  Customer name|\n",
        "        +---------------+\n",
        "        |    Claire Gute|\n",
        "        |    Claire Gute|\n",
        "        |Darrin Van Huff|\n",
        "        +---------------+\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9mJBsBvCkK6",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        df_java.select(\"*\").limit(5).show();\n",
        "        df_java.select(col(\"Customer name\").as(\"CUSTOMER\"),\n",
        "                       column(\"Customer name\")).show(5);\n",
        "\n",
        "        df_java.createOrReplaceTempView(\"df_java_SQL\");\n",
        "        String query = \"SELECT `Customer name` as CUSTOMER FROM df_java_SQL\";\n",
        "        Dataset<Row> selected_row = spark.sql(query);\n",
        "        selected_row.show(5)\n",
        "        ///////////////////////////////////////\n",
        "        +---------------+---------------+\n",
        "        |       CUSTOMER|  Customer name|\n",
        "        +---------------+---------------+\n",
        "        |    Claire Gute|    Claire Gute|\n",
        "        |    Claire Gute|    Claire Gute|\n",
        "        |Darrin Van Huff|Darrin Van Huff|\n",
        "        | Sean O'Donnell| Sean O'Donnell|\n",
        "        | Sean O'Donnell| Sean O'Donnell|\n",
        "        +---------------+---------------+\n",
        "\n",
        "        +---------------+\n",
        "        |       CUSTOMER|\n",
        "        +---------------+\n",
        "        |    Claire Gute|\n",
        "        |    Claire Gute|\n",
        "        |Darrin Van Huff|\n",
        "        | Sean O'Donnell|\n",
        "        | Sean O'Donnell|\n",
        "        +---------------+\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EoPKdPLG-qZ",
        "colab_type": "text"
      },
      "source": [
        "#### Issue 1: space in column and value in df \n",
        "* Solution 1: cover column with ( ` ) when SELECT any column.\n",
        "* Solution 2: replace \"space\" with underscore (_)\n",
        "\n",
        "notice: `WHERE` column `LIKE` `STRING (included \"space\" is allowed)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LEB3hWiHhHG",
        "colab_type": "code",
        "outputId": "ccffb3b3-af26-4aea-f6ac-c4470dfb4be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "query = \"\"\"\n",
        "        SELECT    `Customer Name`\n",
        "        FROM      df_py_SQL\n",
        "        WHERE     `Customer Name` LIKE \"Claire Gute\" \n",
        "        \"\"\"\n",
        "# use `___` for calling  column with space \n",
        "# use \"___\" for calling STRING type\n",
        "\n",
        "spark.sql(query).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|Customer Name|\n",
            "+-------------+\n",
            "|  Claire Gute|\n",
            "|  Claire Gute|\n",
            "|  Claire Gute|\n",
            "|  Claire Gute|\n",
            "|  Claire Gute|\n",
            "+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-uNq7UIG8Fu",
        "colab_type": "code",
        "outputId": "85d37ec5-add7-4ffa-ad86-ea613e8247f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# another solution (not recommend) \n",
        "# replace \"space\" with underscore\n",
        "\n",
        "def under_score_col(df):\n",
        "    for col in df.columns:\n",
        "        df = df.withColumnRenamed(col , col.replace(\" \", \"_\"))\n",
        "    return df\n",
        "\n",
        "def under_score_value(df):\n",
        "    for col in df.columns:\n",
        "        df = df.withColumn(col , F.regexp_replace(F.col(col), \" \", \"_\"))\n",
        "    return df\n",
        "\n",
        "\n",
        "unders_df_py = under_score_value(under_score_col(df_py))\n",
        "unders_df_py.createOrReplaceTempView(\"unders_df_py_SQL\")\n",
        "query = \"\"\"\n",
        "        SELECT    Customer_Name\n",
        "        FROM      unders_df_py_SQL\n",
        "        WHERE     Customer_Name LIKE \"Claire_Gute\" \n",
        "        \"\"\"\n",
        "\n",
        "spark.sql(query).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|Customer_Name|\n",
            "+-------------+\n",
            "|  Claire_Gute|\n",
            "|  Claire_Gute|\n",
            "|  Claire_Gute|\n",
            "|  Claire_Gute|\n",
            "|  Claire_Gute|\n",
            "+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_eK_0LVo8_",
        "colab_type": "text"
      },
      "source": [
        "### ```F.when```, ```df.where``` or ```df.filter``` ???\n",
        "The goal of these three methods are to extract some data from the original but there are slightly different in the result for ```.filter```\n",
        "\n",
        "目的には三つのライブラリーは同じみたいな感じですけど、結果の視点ではちょっと違いがあります。それは```.filter```がもう一つのコラムを作るので、特徴量エンジニアリングみたいなことです。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JZn4kH0i9a_",
        "colab_type": "code",
        "outputId": "f4f5119a-d3f1-431d-bf50-4da42d9b481b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        }
      },
      "source": [
        "# df method (libraries that belong to df directly)\n",
        "# dfのメソッド\n",
        "df_py.where(df_py[\"Profit\"] > 0).select(\"Category\", \"Profit\").show(5)\n",
        "df_py.select(\"Category\", \"Profit\").where(df_py[\"Profit\"] > 0).show(5)\n",
        "df_py.select(\"Category\", \"Profit\").filter(df_py.Profit > 0).show(5)\n",
        "query = \"\"\" SELECT Category, \n",
        "                   Profit\n",
        "            FROM   df_py_SQL\n",
        "            WHERE  Profit > 0 \n",
        "        \"\"\"\n",
        "spark.sql(query).show(5)\n",
        "\n",
        "# F.when needs to exist in select command. (not belong to df directly)\n",
        "# F.whenにはselectの中に入ることが必要です (そのせいでF.whenがselectのメソッドみたいな感じ)\n",
        "df_py.select(\"Category\", \"Profit\", F.when(df_py.Profit > 0, 1).alias(\"my_condition\")).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-------+\n",
            "|       Category| Profit|\n",
            "+---------------+-------+\n",
            "|      Furniture|41.9136|\n",
            "|      Furniture|219.582|\n",
            "|Office Supplies| 6.8714|\n",
            "|Office Supplies| 2.5164|\n",
            "|      Furniture|14.1694|\n",
            "+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+-------+\n",
            "|       Category| Profit|\n",
            "+---------------+-------+\n",
            "|      Furniture|41.9136|\n",
            "|      Furniture|219.582|\n",
            "|Office Supplies| 6.8714|\n",
            "|Office Supplies| 2.5164|\n",
            "|      Furniture|14.1694|\n",
            "+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+-------+\n",
            "|       Category| Profit|\n",
            "+---------------+-------+\n",
            "|      Furniture|41.9136|\n",
            "|      Furniture|219.582|\n",
            "|Office Supplies| 6.8714|\n",
            "|Office Supplies| 2.5164|\n",
            "|      Furniture|14.1694|\n",
            "+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+-------+\n",
            "|       Category| Profit|\n",
            "+---------------+-------+\n",
            "|      Furniture|41.9136|\n",
            "|      Furniture|219.582|\n",
            "|Office Supplies| 6.8714|\n",
            "|Office Supplies| 2.5164|\n",
            "|      Furniture|14.1694|\n",
            "+---------------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+--------+------------+\n",
            "|       Category|  Profit|my_condition|\n",
            "+---------------+--------+------------+\n",
            "|      Furniture| 41.9136|           1|\n",
            "|      Furniture| 219.582|           1|\n",
            "|Office Supplies|  6.8714|           1|\n",
            "|      Furniture|-383.031|        null|\n",
            "|Office Supplies|  2.5164|           1|\n",
            "+---------------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwFRRckAgQe",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        /* We don't need to follow SQL pattern (SFWGHO). scala use OOP inheritance like pySpark */\n",
        "    \n",
        "        df_scala.select(\"Category\", \"Profit\").where($\"Profit\" > 0).show(3)\n",
        "        df_scala.where($\"Profit\" > 0).select(\"Category\", \"Profit\").show(3)\n",
        "        df_scala.select($\"Category\", $\"Profit\").filter($\"Profit\" > 0).show(3)\n",
        "        val query = \"\"\"\n",
        "                    SELECT  Category, \n",
        "                            Profit\n",
        "                    FROM    df_scala_SQL\n",
        "                    WHERE   Profit > 0\n",
        "                    \"\"\"\n",
        "        spark.sql(query).show(3)\n",
        "\n",
        "        import org.apache.spark.sql.functions.when\n",
        "        df_scala.select($\"Category\", $\"Profit\", when($\"Profit\" > 0, 1).alias(\"my_condition\")).show(5)\n",
        "        ///////////////////////////////////////\n",
        "\n",
        "        +---------------+-------+\n",
        "        |       Category| Profit|\n",
        "        +---------------+-------+\n",
        "        |      Furniture|41.9136|\n",
        "        |      Furniture|219.582|\n",
        "        |Office Supplies| 6.8714|\n",
        "        +---------------+-------+\n",
        "        only showing top 3 rows\n",
        "\n",
        "        +---------------+-------+\n",
        "        |       Category| Profit|\n",
        "        +---------------+-------+\n",
        "        |      Furniture|41.9136|\n",
        "        |      Furniture|219.582|\n",
        "        |Office Supplies| 6.8714|\n",
        "        +---------------+-------+\n",
        "        only showing top 3 rows\n",
        "\n",
        "        +---------------+-------+\n",
        "        |       Category| Profit|\n",
        "        +---------------+-------+\n",
        "        |      Furniture|41.9136|\n",
        "        |      Furniture|219.582|\n",
        "        |Office Supplies| 6.8714|\n",
        "        +---------------+-------+\n",
        "        only showing top 3 rows\n",
        "\n",
        "        +---------------+-------+\n",
        "        |       Category| Profit|\n",
        "        +---------------+-------+\n",
        "        |      Furniture|41.9136|\n",
        "        |      Furniture|219.582|\n",
        "        |Office Supplies| 6.8714|\n",
        "        +---------------+-------+\n",
        "        only showing top 3 rows\n",
        "\n",
        "        +---------------+--------+------------+\n",
        "        |       Category|  Profit|my_condition|\n",
        "        +---------------+--------+------------+\n",
        "        |      Furniture| 41.9136|           1|\n",
        "        |      Furniture| 219.582|           1|\n",
        "        |Office Supplies|  6.8714|           1|\n",
        "        |      Furniture|-383.031|        null|\n",
        "        |Office Supplies|  2.5164|           1|\n",
        "        +---------------+--------+------------+\n",
        "        only showing top 5 rows\n",
        "\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf1yE0JxBuec",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "\n",
        "        Dataset<Row> selected_row2 = df_java.select(\"Category\", \"Profit\").where(col(\"Profit\"));\n",
        "        selected_row2.show(3);\n",
        "\n",
        "         //https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/sql/Column.html\n",
        "\n",
        "        // beware that there are a space between in SQL command.\n",
        "        String query2 = \"SELECT `Category`, `Profit` \" +\n",
        "                        \"FROM df_java_SQL \" +\n",
        "                        \"WHERE Profit = 0\";\n",
        "        Dataset<Row> eq_0 = spark.sql(query2);\n",
        "\n",
        "        Dataset<Row> gt_0 = df_java\n",
        "                        .select(\"Category\", \"Profit\")\n",
        "                        .filter(col(\"Profit\").gt(0));\n",
        "\n",
        "        Dataset<Row> gt_0w = df_java\n",
        "                        .select(\"Category\", \"Profit\")\n",
        "                        .where(col(\"Profit\").gt(0));\n",
        "\n",
        "        Dataset<Row> gt_00 = df_java\n",
        "                        .select(\"Category\", \"Profit\")\n",
        "                        .where(col(\"Profit\").$greater(0));\n",
        "\n",
        "        // greater than or equal to\n",
        "        Dataset<Row> geq_0 = df_java\n",
        "                        .select(\"Category\", \"Profit\")\n",
        "                        .where(col(\"Profit\").geq(0));\n",
        "\n",
        "        Dataset<Row> gt_eq_0 = df_java\n",
        "                        .select(\"Category\", \"Profit\")\n",
        "                        .where(col(\"Profit\").$greater$eq(0));\n",
        "\n",
        "        Dataset<Row> gt_0when= df_java.select(col(\"Category\"),\n",
        "                        col(\"Profit\"),\n",
        "                        when(col(\"Profit\").gt(0), 1));\n",
        "\n",
        "\n",
        "        System.out.println( \"eq_0: \" + eq_0.count()+\",\"+eq_0.columns().length + \"\\n\" +\n",
        "                            \"gt_0 (filter): \" + gt_0.count()+\",\"+gt_0.columns().length + \"\\n\" +\n",
        "                            \"gt_0 (where): \" + gt_0w.count()+\",\"+gt_0w.columns().length + \"\\n\" +\n",
        "                            \"gt_0 (when): \" + gt_0when.count()+\",\"+gt_0when.columns().length + \"\\n\" +\n",
        "                            \"$greater_0: \" + gt_00.count()+\",\"+gt_00.columns().length + \"\\n\" +\n",
        "                            \"geq_0: \" + geq_0.count()+\",\"+geq_0.columns().length + \"\\n\" +\n",
        "                            \"$greater$eq: \" + gt_eq_0.count()+\",\"+gt_eq_0.columns().length);\n",
        "\n",
        "        /////////////////////\n",
        "        eq_0: 638,2\n",
        "        gt_0 (filter): 7583,2\n",
        "        gt_0 (where): 7583,2\n",
        "        gt_0 (when): 9994,3\n",
        "        $greater_0: 7583,2\n",
        "        geq_0: 8221,2\n",
        "        $greater$eq: 8221,2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57prvM1tLZFb",
        "colab_type": "text"
      },
      "source": [
        "### Aggregation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B11y-DLWpfRZ",
        "outputId": "f919d620-d418-42b2-973c-6473ba7d6d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# https://stackoverflow.com/questions/47046827/trouble-with-pyspark-round-function\n",
        "a_part = df_py.groupBy([\"Category\"])\\\n",
        "              .agg({\"Profit\":\"sum\"}) # K-V method\n",
        "\n",
        "a_part.show()\n",
        "\n",
        "a_part.withColumn(\"sum(Profit)\", F.round(a_part[\"sum(Profit)\"], 2)).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------------------+\n",
            "|       Category|       sum(Profit)|\n",
            "+---------------+------------------+\n",
            "|Office Supplies|120632.87839999991|\n",
            "|      Furniture| 19686.42720000003|\n",
            "|     Technology|145388.29659999989|\n",
            "+---------------+------------------+\n",
            "\n",
            "+---------------+-----------+\n",
            "|       Category|sum(Profit)|\n",
            "+---------------+-----------+\n",
            "|Office Supplies|  120632.88|\n",
            "|      Furniture|   19686.43|\n",
            "|     Technology|   145388.3|\n",
            "+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cshuJ7WUDJH7",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        // K-V method\n",
        "        import org.apache.spark.sql.functions.map\n",
        "        df_scala.groupBy($\"Category\")\n",
        "                .agg(Map(\"Profit\" -> \"sum\"))\n",
        "                .show()\n",
        "\n",
        "        // general method\n",
        "        df_scala.groupBy($\"Category\")\n",
        "                .agg(sum($\"Profit\"))\n",
        "                .show()\n",
        "\n",
        "        //////////////////////////////////\n",
        "        +---------------+------------------+\n",
        "        |       Category|       sum(Profit)|\n",
        "        +---------------+------------------+\n",
        "        |Office Supplies|120632.87839999991|\n",
        "        |      Furniture| 19686.42720000003|\n",
        "        |     Technology|145388.29659999989|\n",
        "        +---------------+------------------+\n",
        "\n",
        "        +---------------+------------------+\n",
        "        |       Category|       sum(Profit)|\n",
        "        +---------------+------------------+\n",
        "        |Office Supplies|120632.87839999991|\n",
        "        |      Furniture| 19686.42720000003|\n",
        "        |     Technology|145388.29659999989|\n",
        "        +---------------+------------------+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7UQmx29DZfp",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        Dataset<Row> agg1 = df_java.groupBy(col(\"Category\"))\n",
        "                                   .agg(sum(col(\"Profit\")));\n",
        "        agg1.show(3);\n",
        "\n",
        "        ////////////////////////////////////////\n",
        "        +---------------+------------------+\n",
        "        |       Category|       sum(Profit)|\n",
        "        +---------------+------------------+\n",
        "        |Office Supplies|120632.87839999991|\n",
        "        |      Furniture| 19686.42720000003|\n",
        "        |     Technology|145388.29659999989|\n",
        "        +---------------+------------------+\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyvmCY0NzE2-",
        "colab_type": "code",
        "outputId": "bf1fe3aa-a145-4033-c957-65c9520c3e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Multiple aggregations\n",
        "\n",
        "df_py.groupBy(\"Category\")\\\n",
        "     .agg(F.min(\"Profit\"),\n",
        "          F.max(\"Profit\"),\n",
        "          F.sum(\"Profit\"))\\\n",
        "     .show()\n",
        "\n",
        "     \n",
        "query = \"\"\" \n",
        "        SELECT    Category, \n",
        "                  MIN(Profit), \n",
        "                  MAX(Profit), \n",
        "                  SUM(Profit)\n",
        "        FROM      df_py_SQL\n",
        "        GROUP BY  Category\n",
        "        \"\"\"\n",
        "spark.sql(query).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-----------+-----------+------------------+\n",
            "|       Category|min(Profit)|max(Profit)|       sum(Profit)|\n",
            "+---------------+-----------+-----------+------------------+\n",
            "|Office Supplies|    -0.2098|    99.9408|120632.87839999991|\n",
            "|      Furniture|    -0.3398|     99.432| 19686.42720000003|\n",
            "|     Technology|    -0.0895|    98.2722|145388.29659999989|\n",
            "+---------------+-----------+-----------+------------------+\n",
            "\n",
            "+---------------+-----------+-----------+---------------------------+\n",
            "|       Category|min(Profit)|max(Profit)|sum(CAST(Profit AS DOUBLE))|\n",
            "+---------------+-----------+-----------+---------------------------+\n",
            "|Office Supplies|    -0.2098|    99.9408|         120632.87839999991|\n",
            "|      Furniture|    -0.3398|     99.432|          19686.42720000003|\n",
            "|     Technology|    -0.0895|    98.2722|         145388.29659999989|\n",
            "+---------------+-----------+-----------+---------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5tcT1MZpgoC",
        "colab_type": "text"
      },
      "source": [
        "### Rounding dataframe via \n",
        "```from pyspark.sql import functions as F```\n",
        "\n",
        "`F.round`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFmEUylZ3dg4",
        "colab_type": "code",
        "outputId": "c77f8230-e9b4-4c71-b470-ecab7b0e7763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "# Single aggregation\n",
        "\n",
        "# K-V method\n",
        "df_py.groupBy([\"Category\"])\\\n",
        "     .agg({\"Profit\":\"sum\"})\\\n",
        "     .show()\n",
        "\n",
        "# genral method\n",
        "from pyspark.sql import functions as F\n",
        "df_py.groupBy([\"Category\"])\\\n",
        "     .agg(F.sum(\"Profit\"))\\\n",
        "     .show()\n",
        "\n",
        "# For Spark 2.4, this agg of code is not working now but it is work for Spark 1.XX\n",
        "# http://sinhrks.hatenablog.com/entry/2015/04/29/085353\n",
        "df_py.groupby([\"Category\"]).sum().show() \n",
        "# however, for .count() is still work. what is going on?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------------------+\n",
            "|       Category|       sum(Profit)|\n",
            "+---------------+------------------+\n",
            "|Office Supplies|120632.87839999991|\n",
            "|      Furniture| 19686.42720000003|\n",
            "|     Technology|145388.29659999989|\n",
            "+---------------+------------------+\n",
            "\n",
            "+---------------+------------------+\n",
            "|       Category|       sum(Profit)|\n",
            "+---------------+------------------+\n",
            "|Office Supplies|120632.87839999991|\n",
            "|      Furniture| 19686.42720000003|\n",
            "|     Technology|145388.29659999989|\n",
            "+---------------+------------------+\n",
            "\n",
            "+---------------+\n",
            "|       Category|\n",
            "+---------------+\n",
            "|Office Supplies|\n",
            "|      Furniture|\n",
            "|     Technology|\n",
            "+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4Uzc9UD8jA",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        val a_part = df_scala.groupBy($\"Category\")\n",
        "                            .agg(Map(\"Profit\" -> \"sum\"))\n",
        "                        \n",
        "\n",
        "        a_part.withColumn(\"sum(Profit)\", \n",
        "                        round($\"sum(Profit)\", 2)\n",
        "                        ).show(3)\n",
        "\n",
        "        /////////////////////////\n",
        "        +---------------+-----------+\n",
        "        |       Category|sum(Profit)|\n",
        "        +---------------+-----------+\n",
        "        |Office Supplies|  120632.88|\n",
        "        |      Furniture|   19686.43|\n",
        "        |     Technology|   145388.3|\n",
        "        +---------------+-----------+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLyviCPhFHtI",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        Dataset<Row> agg1 = df_java.groupBy(col(\"Category\"))\n",
        "                                   .agg(sum(col(\"Profit\")));\n",
        "\n",
        "        Dataset<Row> r_agg1 = agg1.withColumn(\"sum(Profit)\",\n",
        "                                    round(col(\"sum(Profit)\"), 2));\n",
        "        r_agg1.show(3);\n",
        "        \n",
        "        ///////////////////////////////////\n",
        "        +---------------+-----------+\n",
        "        |       Category|sum(Profit)|\n",
        "        +---------------+-----------+\n",
        "        |Office Supplies|  120632.88|\n",
        "        |      Furniture|   19686.43|\n",
        "        |     Technology|   145388.3|\n",
        "        +---------------+-----------+\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aSV0XAphPwY",
        "colab_type": "text"
      },
      "source": [
        "### Window functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wxEVG9YPhdp",
        "colab_type": "code",
        "outputId": "229f3443-61c1-4be0-b9d6-76a5b7197a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "query = \"\"\"\n",
        "        SELECT     `Customer name`,\n",
        "                   `Postal Code`, \n",
        "                   `Region`,\n",
        "                   `Quantity`,\n",
        "                    SUM(`Quantity`) OVER (PARTITION BY `Customer name`) AS `SUM quantity of each customer`,\n",
        "                    Max(`Quantity`) OVER (PARTITION BY `Region`) AS `MAX quantity of each region`\n",
        "         FROM       df_py_SQL\n",
        "         WHERE     `Postal Code` > 80000\n",
        "         ORDER BY  `Customer name`\n",
        "         \"\"\"\n",
        "\n",
        "\n",
        "print(\"how many items had been bought by customer who are living in the west region??? \") # Assume that postal_code of west reg is greater than 80k \n",
        "spark.sql(query).show(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how many items had been bought by customer who are living in the west region??? \n",
            "+--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
            "| Customer name|Postal Code|Region|Quantity|SUM quantity of each customer|MAX quantity of each region|\n",
            "+--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
            "| Aaron Bergman|      98103|  West|       3|                          7.0|                     98.352|\n",
            "| Aaron Bergman|      98103|  West|       3|                          7.0|                     98.352|\n",
            "| Aaron Bergman|      98103|  West|       1|                          7.0|                     98.352|\n",
            "| Aaron Hawkins|      94109|  West|       4|                         23.0|                     98.352|\n",
            "| Aaron Hawkins|      90004|  West|       2|                         23.0|                     98.352|\n",
            "| Aaron Hawkins|      94122|  West|       2|                         23.0|                     98.352|\n",
            "| Aaron Hawkins|      94122|  West|       9|                         23.0|                     98.352|\n",
            "| Aaron Hawkins|      90004|  West|       6|                         23.0|                     98.352|\n",
            "|Aaron Smayling|      94110|  West|       2|                         12.0|                     98.352|\n",
            "|Aaron Smayling|      97756|  West|       7|                         12.0|                     98.352|\n",
            "+--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okIhyjWaIpag",
        "colab_type": "text"
      },
      "source": [
        "    // scala\n",
        "        val query = \"\"\"\n",
        "                SELECT  `Customer name`,\n",
        "                        `Postal Code`, \n",
        "                        `Region`,\n",
        "                        `Quantity`,\n",
        "                        SUM(`Quantity`) OVER (PARTITION BY `Customer name`) AS `SUM quantity of each customer`,\n",
        "                        Max(`Quantity`) OVER (PARTITION BY `Region`) AS `MAX quantity of each region`\n",
        "                FROM       df_scala_SQL\n",
        "                WHERE     `Postal Code` > 80000\n",
        "                ORDER BY  `Customer name`\n",
        "                \"\"\"\n",
        "\n",
        "        how many items had been bought by customer who are living in the west region??? \n",
        "        +--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
        "        | Customer name|Postal Code|Region|Quantity|SUM quantity of each customer|MAX quantity of each region|\n",
        "        +--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
        "        | Aaron Bergman|      98103|  West|       3|                          7.0|                     98.352|\n",
        "        | Aaron Bergman|      98103|  West|       3|                          7.0|                     98.352|\n",
        "        | Aaron Bergman|      98103|  West|       1|                          7.0|                     98.352|\n",
        "        | Aaron Hawkins|      90004|  West|       2|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      94109|  West|       4|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      90004|  West|       6|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      94122|  West|       2|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      94122|  West|       9|                         23.0|                     98.352|\n",
        "        |Aaron Smayling|      94110|  West|       2|                         12.0|                     98.352|\n",
        "        |Aaron Smayling|      91104|  West|       3|                         12.0|                     98.352|\n",
        "        +--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
        "\n",
        "println(\"how many items had been bought by customer who are living in the west region??? \") \n",
        "// Assume that postal_code of west reg is greater than 80k \n",
        "spark.sql(query).show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOamyQehGGba",
        "colab_type": "text"
      },
      "source": [
        "    // java\n",
        "        String query3 =\n",
        "                \" SELECT   `Customer name`, \" +\n",
        "                        \" `Postal Code`,\" +\n",
        "                        \" `Region`,\" +\n",
        "                        \" `Quantity`,\" +\n",
        "                        \" SUM(`Quantity`) OVER (PARTITION BY `Customer name`) AS `SUM quantity of each customer`, \" +\n",
        "                        \" Max(`Quantity`) OVER (PARTITION BY `Region`) AS `MAX quantity of each region` \" +\n",
        "                        \" FROM     df_java_SQL \" +\n",
        "                        \" WHERE    `Postal Code` > 80000 \" +\n",
        "                        \" ORDER BY `Customer name` \";\n",
        "\n",
        "        // Assume that postal_code of west reg is greater than 80k\n",
        "        System.out.println(\"how many items had been bought by customer who are living in the west region??? \");\n",
        "        spark.sql(query3).show(10);\n",
        "        /////////////////////////////////////\n",
        "\n",
        "        how many items had been bought by customer who are living in the west region??? \n",
        "        +--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
        "        | Customer name|Postal Code|Region|Quantity|SUM quantity of each customer|MAX quantity of each region|\n",
        "        +--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
        "        | Aaron Bergman|      98103|  West|       3|                          7.0|                     98.352|\n",
        "        | Aaron Bergman|      98103|  West|       3|                          7.0|                     98.352|\n",
        "        | Aaron Bergman|      98103|  West|       1|                          7.0|                     98.352|\n",
        "        | Aaron Hawkins|      94109|  West|       4|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      90004|  West|       2|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      94122|  West|       2|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      94122|  West|       9|                         23.0|                     98.352|\n",
        "        | Aaron Hawkins|      90004|  West|       6|                         23.0|                     98.352|\n",
        "        |Aaron Smayling|      94110|  West|       2|                         12.0|                     98.352|\n",
        "        |Aaron Smayling|      97756|  West|       7|                         12.0|                     98.352|\n",
        "        +--------------+-----------+------+--------+-----------------------------+---------------------------+\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E2PD_0sftjU",
        "colab_type": "text"
      },
      "source": [
        " Aggregate all columns of a dataframe at once\n",
        "   \n",
        "    https://stackoverflow.com/questions/56256785/pyspark-aggregate-all-columns-of-a-dataframe-at-once?noredirect=1&lq=1\n",
        "    https://stackoverflow.com/questions/33882894/spark-sql-apply-aggregate-functions-to-a-list-of-columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHPv2Jb9dHyO",
        "colab_type": "text"
      },
      "source": [
        "### Aggregaton function vs Window function\n",
        "![alt text](https://static.packt-cdn.com/products/9781783989188/graphics/B01781_06_02.jpg)\n",
        "\n",
        "cr. https://static.packt-cdn.com/products/9781783989188/graphics/B01781_06_02.jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmdO-0mdjlm2",
        "colab_type": "code",
        "outputId": "c5d4839b-5f4d-4ed6-f807-b9c8db5e8b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "print(\"how much contribution of each customer (across customer) in each region when return items(-) are not concerned\")\n",
        "query = \"\"\"SELECT  `Customer name`,\n",
        "                   Profit,\n",
        "                   Region,\n",
        "                   Quantity,\n",
        "                   Discount, \n",
        "                   MAX(Profit) OVER (PARTITION BY `Customer name`) AS MAX_profit_each_customer, \\\n",
        "                   SUM(Profit) OVER (PARTITION BY `Customer name`) AS SUM_profit_each_customer, \\\n",
        "                   SUM(Profit) OVER (PARTITION BY Region) AS SUM_profit_each_region \\\n",
        "         FROM      df_py_SQL \n",
        "         WHERE     Profit > 0 \n",
        "         ORDER BY  `Customer name`\"\"\"\n",
        "\n",
        "\n",
        "spark.sql(query).show(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how much contribution of each customer (across customer) in each region when return items(-) are not concerned\n",
            "+--------------+--------+-------+--------+--------+------------------------+------------------------+----------------------+\n",
            "| Customer name|  Profit| Region|Quantity|Discount|MAX_profit_each_customer|SUM_profit_each_customer|SUM_profit_each_region|\n",
            "+--------------+--------+-------+--------+--------+------------------------+------------------------+----------------------+\n",
            "| Aaron Bergman|  5.4801|   West|       1|     0.2|                 62.1544|      131.87130000000002|    129822.30940000013|\n",
            "| Aaron Bergman|  4.6644|   West|       3|       0|                 62.1544|      131.87130000000002|    129822.30940000013|\n",
            "| Aaron Bergman| 54.7136|Central|       2|       0|                 62.1544|      131.87130000000002|     95473.11760000007|\n",
            "| Aaron Bergman| 62.1544|Central|       2|       0|                 62.1544|      131.87130000000002|     95473.11760000007|\n",
            "| Aaron Bergman|  4.8588|   West|       3|       0|                 62.1544|      131.87130000000002|    129822.30940000013|\n",
            "| Aaron Hawkins|    3.84|   West|       2|       0|                   8.694|                365.2152|    129822.30940000013|\n",
            "| Aaron Hawkins|  75.168|   West|       9|     0.2|                   8.694|                365.2152|    129822.30940000013|\n",
            "| Aaron Hawkins|121.4416|   East|       8|       0|                   8.694|                365.2152|           140736.7344|\n",
            "| Aaron Hawkins|   8.694|   East|       3|       0|                   8.694|                365.2152|           140736.7344|\n",
            "| Aaron Hawkins|  61.389|   West|       2|       0|                   8.694|                365.2152|    129822.30940000013|\n",
            "| Aaron Hawkins|  18.528|   West|       4|     0.2|                   8.694|                365.2152|    129822.30940000013|\n",
            "| Aaron Hawkins| 11.4741|   East|       3|       0|                   8.694|                365.2152|           140736.7344|\n",
            "| Aaron Hawkins| 20.9592|   West|       6|     0.2|                   8.694|                365.2152|    129822.30940000013|\n",
            "| Aaron Hawkins|  38.038|  South|       7|       0|                   8.694|                365.2152|     73979.04859999991|\n",
            "| Aaron Hawkins|   2.338|   East|       7|     0.2|                   8.694|                365.2152|           140736.7344|\n",
            "| Aaron Hawkins|  3.3453|   East|       3|     0.2|                   8.694|                365.2152|           140736.7344|\n",
            "|Aaron Smayling| 32.2322|   East|      11|       0|                  84.294|                 137.077|           140736.7344|\n",
            "|Aaron Smayling|    7.85|  South|       2|       0|                  84.294|                 137.077|     73979.04859999991|\n",
            "|Aaron Smayling|  84.294|   West|       2|    0.15|                  84.294|                 137.077|    129822.30940000013|\n",
            "|Aaron Smayling| 12.7008|Central|       7|     0.2|                  84.294|                 137.077|     95473.11760000007|\n",
            "+--------------+--------+-------+--------+--------+------------------------+------------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOtyuKFiRYjZ",
        "colab_type": "text"
      },
      "source": [
        "### Time series wrangling\n",
        "\n",
        "notice: column type is very importance in time series wrangling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93hki9c1OqOL",
        "colab_type": "code",
        "outputId": "0dae412b-bbce-4703-8faf-a23dacb1d609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "df_py.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Row ID: string (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Order Date: string (nullable = true)\n",
            " |-- Ship Date: string (nullable = true)\n",
            " |-- Ship Mode: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Segment: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Postal Code: string (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Product ID: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Sub-Category: string (nullable = true)\n",
            " |-- Product Name: string (nullable = true)\n",
            " |-- Sales: string (nullable = true)\n",
            " |-- Quantity: string (nullable = true)\n",
            " |-- Discount: string (nullable = true)\n",
            " |-- Profit: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "filCCBImOqJY",
        "colab_type": "code",
        "outputId": "1e8ea084-55b2-4891-de8c-f23ff4e1d6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# Timestamp format\n",
        "df_py.select(F.to_timestamp(df_py[\"Ship Date\"], 'yyyy-MM-dd').alias(\"Ship Date\")).show(3)\n",
        "\n",
        "# Date format\n",
        "df_py.select(F.to_date(df_py[\"Ship Date\"])).show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|          Ship Date|\n",
            "+-------------------+\n",
            "|2017-11-11 00:00:00|\n",
            "|2017-11-11 00:00:00|\n",
            "|2017-06-16 00:00:00|\n",
            "+-------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+--------------------+\n",
            "|to_date(`Ship Date`)|\n",
            "+--------------------+\n",
            "|          2017-11-11|\n",
            "|          2017-11-11|\n",
            "|          2017-06-16|\n",
            "+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeG4M63K0nhJ",
        "colab_type": "text"
      },
      "source": [
        "Add modified column in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnqBvsiMb6QK",
        "colab_type": "code",
        "outputId": "20a97159-11d1-4609-d528-ba3404f1ffba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# new_df_py = df_py.drop(\"Ship Date\", \"Order Date\")\n",
        "new_df_py = df_py.withColumn(\"Ship Date (date)\", F.to_date(df_py[\"Ship Date\"]))\\\n",
        "                 .withColumn(\"Order Date (date)\", F.to_date(df_py[\"Order Date\"]))\\\n",
        "                 .drop(\"Order Date\", \"Ship Date\") # drop the original\n",
        "                 \n",
        "new_df_py  = new_df_py.withColumnRenamed(\"Order Date (date)\", \"Order Date\")\\\n",
        "                      .withColumnRenamed(\"Ship Date (date)\", \"Ship Date\")\n",
        "\n",
        "# sort columns\n",
        "    # new_df_py = new_df_py.select(sorted(new_df_py.columns))     \n",
        "\n",
        "new_df_py.printSchema()            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Row ID: string (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Ship Mode: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Customer Name: string (nullable = true)\n",
            " |-- Segment: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Postal Code: string (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Product ID: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Sub-Category: string (nullable = true)\n",
            " |-- Product Name: string (nullable = true)\n",
            " |-- Sales: string (nullable = true)\n",
            " |-- Quantity: string (nullable = true)\n",
            " |-- Discount: string (nullable = true)\n",
            " |-- Profit: string (nullable = true)\n",
            " |-- Ship Date: date (nullable = true)\n",
            " |-- Order Date: date (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}