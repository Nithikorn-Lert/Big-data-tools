{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark - Several ways to [select] data in Spark dataframe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7yB5QEwFeOR",
        "colab_type": "text"
      },
      "source": [
        "Goals: To access the data in Spark dataframe.\n",
        "\n",
        "Since there are several ways to access the data in Spark dataframe. In this note book we show how to access the data in several method in three programming languages which are Python, Scala and Java.\n",
        "\n",
        "Because Scala and Java are not compatible with Jupyter notebook. So, we use the result from IntelliJ instead.\n",
        "\n",
        "Ref:\n",
        "- [1] https://x1.inkenkun.com/archives/1114#SELECT\n",
        "- [2] http://mogile.web.fc2.com/spark/spark210/sql-programming-guide.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOSc-mXClb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71yx1CQm05wi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b570b8ce-08fc-47a0-b178-d804b3ed91ec"
      },
      "source": [
        "!git clone https://github.com/damiannolan/iris-neural-network.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'iris-neural-network'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Total 77 (delta 0), reused 0 (delta 0), pack-reused 77\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOy2xT0dCnX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession\\\n",
        "         .builder\\\n",
        "         .master(\"local[*]\")\\\n",
        "         .appName(\"sample_app\")\\\n",
        "         .enableHiveSupport()\\\n",
        "         .getOrCreate()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdy0CcqrOxfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_py = spark.read.csv(\"iris-neural-network/iris-data-set.csv\", header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbhF2M8tQDhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# register SQL-command df like table named df_SQL\n",
        "df_py.createOrReplaceTempView(\"df_py_SQL\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXVLbkVfO_HK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "3a672937-bda5-4da5-b2a0-03f0af3f85cd"
      },
      "source": [
        "df_py.show(5)\n",
        "print(\"=============================================\")\n",
        "spark.sql(\"SELECT * FROM df_py_SQL LIMIT 5\").show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "=============================================\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ9Xr-H6eS9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "61b35cef-4403-4445-d8c6-a7c90cfd791d"
      },
      "source": [
        "df_py.select(\"*\").show(5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wNIQBtterhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "f76a3b00-b33e-418a-80fe-c10a0a8d2ff1"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_py.select(F.col(\"sepal_length\")).show(5) # spark approach\n",
        "df_py.select(df_py[\"sepal_length\"], df_py[0]).show(5) # pythonic approachs"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|sepal_length|\n",
            "+------------+\n",
            "|         5.1|\n",
            "|         4.9|\n",
            "|         4.7|\n",
            "|         4.6|\n",
            "|         5.0|\n",
            "+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+------------+\n",
            "|sepal_length|sepal_length|\n",
            "+------------+------------+\n",
            "|         5.1|         5.1|\n",
            "|         4.9|         4.9|\n",
            "|         4.7|         4.7|\n",
            "|         4.6|         4.6|\n",
            "|         5.0|         5.0|\n",
            "+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oza8iJLrFLv2",
        "colab_type": "text"
      },
      "source": [
        "In Scala and Java the name of columns were changed into SepalLength and so on. However It 's not our point in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCKthHv6b5RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% Scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.SparkContext\n",
        "import org.apache.spark.SparkConf\n",
        "\n",
        "import org.apache.spark.sql.functions.column\n",
        "import org.apache.spark.sql.functions.col\n",
        "\n",
        "val spark = SparkSession\n",
        "    .builder()\n",
        "    .master(\"local[*]\")\n",
        "    .appName(\"sample_app\")\n",
        "    .getOrCreate()\n",
        "    import spark.implicits._\n",
        "\n",
        "val df_scala = spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .load(\"C:/Users/CU - teminal/Desktop/Spark/iris.csv\")\n",
        "\n",
        "df_scala.select($\"SepalLength\", // several ways to access the data in scala spark DF\n",
        "                 'SepalLength,\n",
        "                 col(\"SepalLength\"), // spark.sql.functions.col\n",
        "                 column(\"SepalLength\"), // spark.sql.functions.column\n",
        "                \n",
        "                 'SepalLength as 'column_one)\n",
        "        .show(5) // AS\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "equivalent to\n",
        "    spark.sql(\"SELECT SepalLength, \" +\n",
        "\n",
        "      \"                       SepalLength AS column_one\" +\n",
        "      \"                       FROM df_scala_SQL \" +\n",
        "      \"                       LIMIT 5\").show()\n",
        "\"\"\"\n",
        "\n",
        "# +-----------+-----------+-----------+-----------+----------+\n",
        "# |SepalLength|SepalLength|SepalLength|SepalLength|column_one|\n",
        "# +-----------+-----------+-----------+-----------+----------+\n",
        "# |        5.1|        5.1|        5.1|        5.1|       5.1|\n",
        "# |        4.9|        4.9|        4.9|        4.9|       4.9|\n",
        "# |        4.7|        4.7|        4.7|        4.7|       4.7|\n",
        "# |        4.6|        4.6|        4.6|        4.6|       4.6|\n",
        "# |        5.0|        5.0|        5.0|        5.0|       5.0|\n",
        "# +-----------+-----------+-----------+-----------+----------+\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlIvA4K1b6kA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% Java\n",
        "\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "\n",
        "import org.apache.spark.api.java.JavaRDD;\n",
        "import org.apache.spark.api.java.JavaSparkContext;\n",
        "import org.apache.spark.SparkConf;\n",
        "\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "\n",
        "import static org.apache.spark.sql.functions.col;\n",
        "import static org.apache.spark.sql.functions.column;\n",
        "\n",
        "SparkSession spark = SparkSession\n",
        "                .builder()\n",
        "                .master(\"local[*]\")\n",
        "                .appName(\"sample_app\")\n",
        "                .getOrCreate();\n",
        "\n",
        "Dataset<Row> df_java = spark.read()\n",
        "        .format(\"csv\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .load(\"C:/Users/CU - teminal/Desktop/Spark/iris.csv\");\n",
        "\n",
        "df_java.select(col(\"SepalLength\"),\n",
        "               column(\"SepalLength\"))\n",
        "        .show(5);\n",
        "    \n",
        "# +-----------+-----------+\n",
        "# |SepalLength|SepalLength|\n",
        "# +-----------+-----------+\n",
        "# |        5.1|        5.1|\n",
        "# |        4.9|        4.9|\n",
        "# |        4.7|        4.7|\n",
        "# |        4.6|        4.6|\n",
        "# |        5.0|        5.0|\n",
        "# +-----------+-----------+"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}